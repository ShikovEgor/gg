{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os.path as osp\n",
    "import pathlib\n",
    "import math\n",
    "\n",
    "from typing import Union, Tuple\n",
    "from typing import List, Optional, Set, get_type_hints\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.typing import OptPairTensor, Adj, Size\n",
    "from torch_scatter import gather_csr, scatter, segment_csr\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Sequential, ReLU, Linear, Dropout, BatchNorm1d\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# from src.data_preparation import RWDataset, data_gen_e_aug\n",
    "# from src.models import GraphNet, add_weight_decay\n",
    "# from src.utils import create_nx_graph,create_gt_graph, draw_deg_distr, relabel, init_graph\n",
    "# from src.utils import sel_start_node, sel_start_node_old, get_errors\n",
    "# from src.train import LabelSmoothing\n",
    "# from src.utils import NodeSelector\n",
    "from src.model import GPT"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def load_PYG_datasets(path, d_name = 'PROTEINS'):\n",
    "    pth = path\n",
    "    path = osp.join(pathlib.Path().absolute(), pth , d_name)\n",
    "    dset = TUDataset(path, d_name)\n",
    "    \n",
    "    #sozdal otdelno pole features, a v pole x pomestil indexi - t.k. vektora vershin u nas menyayutsya\n",
    "    dset.features = dset.data.x \n",
    "    dset.data.x = torch.arange(dset.data.x.shape[0])\n",
    "    return dset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dset = load_PYG_datasets(path='./data/proteins', d_name='PROTEINS')\n",
    "#dset = RWDataset('')  random walks for Cora"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "BSIZE = 16\n",
    "train_loader = DataLoader(dset, batch_size=BSIZE, shuffle=False)# , exclude_keys=['x']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def data_gen_e_aug(train_loader, slices, batch_size = 4, step = 1):\n",
    "    r\"\"\" berem skleennie graphi, iteriruem po vershinam, stroim augmented graph\n",
    "\n",
    "    Args:\n",
    "        train_loader: PyG DataLoader\n",
    "        slices: indexy reber, po kotorim skleivali nabor graphov\n",
    "        batch_size:\n",
    "        step: po skolko reber narashivaem graph, obichno 1\n",
    "    Outputs:\n",
    "        vi: indexi vershin v ishodnom graphe\n",
    "        graph: augmented graph\n",
    "        edges: priroshennie rebra\n",
    "    \"\"\"\n",
    "    \n",
    "    for ib, data in enumerate(train_loader):\n",
    "        e_ptr = slices[ib*batch_size:(ib+1)*batch_size+1]\n",
    "        e_ptr = e_ptr - e_ptr[0]\n",
    "        szs = e_ptr[1:]-e_ptr[:-1]\n",
    "        e_ind_start = e_ptr[1:]-szs.min()+1\n",
    "        visited_e = torch.full((e_ptr[-1],), False, dtype = torch.bool)\n",
    "        for i in range(e_ind_start.shape[0]):\n",
    "            visited_e[e_ptr[:-1][i] :e_ind_start[i]] = True # setting emask True for edges in graph\n",
    "        edges_num = torch.arange(e_ptr[-1])\n",
    "\n",
    "        visited_v = torch.full((data.ptr[-1],), False)\n",
    "        visited_v[torch.unique(data.edge_index[:, visited_e])] = True\n",
    "        \n",
    "        vert_ind = []\n",
    "        last_ind_v = torch.arange(data.ptr[-1])\n",
    "        last_ind_max = data.ptr[-1].item()        \n",
    "        ei_dict = defaultdict(set)\n",
    "        for e in data.edge_index[:, visited_e].T:\n",
    "            e = e.to(dtype=torch.long)\n",
    "            ei_dict[e[0].item()].add(e[1].item()) \n",
    "            ei_dict[e[1].item()].add(e[0].item()) \n",
    "        edge_added = []\n",
    "        \n",
    "        for i in range(data.edge_index.shape[1]): # max number of iterations, usually we stop earlier\n",
    "            if torch.all(visited_e):\n",
    "                break           \n",
    "                  \n",
    "            e1_mask = (visited_v[data.edge_index[0]] | visited_v[data.edge_index[1]]) & ~visited_e # Source in graph\n",
    "            nnedges = edges_num[e1_mask]\n",
    "            e1_ind = []\n",
    "            for j in range(1, e_ptr.shape[0]):\n",
    "                mmask = (nnedges < e_ptr[j]) & (nnedges >= e_ptr[j-1])\n",
    "                e1_ind.append(nnedges[mmask][:step])         \n",
    "            e1_ind = torch.cat(e1_ind)\n",
    "            edges_1 = data.edge_index[:, e1_ind]\n",
    "                        \n",
    "            for e in edges_1.T: \n",
    "                e_reind = []\n",
    "                edge_added.append(last_ind_v[e].view(-1,1))\n",
    "                for iv in (True,False):\n",
    "                    ind = last_ind_v[e[int(iv)]].item()\n",
    "                    if ind in ei_dict.keys():\n",
    "                        ind = last_ind_max\n",
    "                        vert_ind.append(e[int(iv)])\n",
    "                        last_ind_v[e[int(iv)]] = ind\n",
    "                        ei_dict[ind] = ei_dict[e[int(iv)].item()]   \n",
    "                        last_ind_max += 1  \n",
    "                    e_reind.append(ind)\n",
    "                    \n",
    "                ei_dict[e_reind[0]].add(e_reind[1]) #sporno\n",
    "                ei_dict[e_reind[1]].add(e_reind[0])\n",
    "        \n",
    "            # selecting source-target when both vertices in graph            \n",
    "            visited_e[e1_ind] = True  \n",
    "            visited_v[edges_1.view(-1)] = True\n",
    "        \n",
    "        edge_index = []\n",
    "        for k,v in ei_dict.items():\n",
    "            e = torch.tensor(list(v)).view(1,-1)\n",
    "            edge_index.append(torch.cat((e, torch.full_like(e, k)), dim=0))\n",
    "\n",
    "        vert_index = torch.cat((torch.arange(data.ptr[-1]),torch.Tensor(vert_ind))).to(dtype=torch.long) \n",
    "        \n",
    "        yield  data.x[vert_index], torch.cat(edge_added, dim=1),\\\n",
    "                torch.cat(edge_index, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_gen = data_gen_e_aug(train_loader, dset.slices['edge_index'], batch_size = BSIZE,step = 1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "device = torch.device('cpu')\n",
    "    \n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "        \n",
    "arch0 = {'input_dim': dset.features.shape[1], \n",
    "        'hidden_dim': 32,\n",
    "        'num_layers': 2,\n",
    "        'num_heads': 8,\n",
    "        'attn_pdrop': 0.5,\n",
    "        'resid_pdrop': 0.5,\n",
    "        'embd_pdrop': 0.5,\n",
    "        'gnn_pdrop': 0.5,\n",
    "        'num_gnn_layers': 1,\n",
    "        'mlp_pdrop': 0.5}\n",
    "conf = objectview(arch0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "class GraphNet(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, dropout_p = 0.2):\n",
    "        super(GraphNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(num_features=hidden_dim))\n",
    "            \n",
    "    def forward(self, v_ind, features, edge_index):\n",
    "        x = features[v_ind]\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SRAN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SRAN, self).__init__()\n",
    "\n",
    "        self.input_dim = config.input_dim\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.num_layers = config.num_layers\n",
    "        self.num_heads = config.num_heads\n",
    "        self.attn_pdrop = config.attn_pdrop\n",
    "        self.resid_pdrop = config.resid_pdrop\n",
    "        self.embd_pdrop = config.embd_pdrop\n",
    "        self.gnn_pdrop  = config.gnn_pdrop\n",
    "        self.num_gnn_layers = config.num_gnn_layers\n",
    "        self.mlp_pdrop = config.mlp_pdrop\n",
    "\n",
    "        self.lin_inp = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hidden_dim)\n",
    "#             nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "#             nn.Dropout(self.mlp_pdrop)\n",
    "        )\n",
    "\n",
    "        self.gnn = GraphNet(\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_layers=self.num_gnn_layers,\n",
    "        )\n",
    "\n",
    "        self.gpt = GPT(\n",
    "            hidden_dim=2 * self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            num_heads=self.num_heads,\n",
    "            attn_pdrop=self.attn_pdrop,\n",
    "            resid_pdrop=self.resid_pdrop,\n",
    "            embd_pdrop=self.embd_pdrop\n",
    "        )\n",
    "        \n",
    "    def forward(self, v_ind, features, edge_index, edges):\n",
    "        feat = self.lin_inp(features)\n",
    "        gnn_feat = self.gnn(v_ind, feat, edge_index)\n",
    "\n",
    "        src, dst = edges\n",
    "        gpt_input = torch.cat([\n",
    "            gnn_feat[src],\n",
    "            gnn_feat[dst]], dim=1).unsqueeze(0)\n",
    "\n",
    "        edge_embs = self.gpt(gpt_input)\n",
    "\n",
    "        return edge_embs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         features = self.lin_inp(features) \n",
    "        \n",
    "#         h_gnn = self.gnn(v_ind, features, edge_index)\n",
    "#         source_ind, target_ind = edges\n",
    "#         h_source, h_target = gnn_emb[source_ind], gnn_emb[target_ind]\n",
    "        \n",
    "#         h_next_e = self.gpt(torch.cat((h_source, h_target)))\n",
    "        \n",
    "# #         y_source = self.choice(self.mlp_y_s(h_next_e), h_source) # сместить на 1 позицию\n",
    "# #         z_target = self.mlp_y_t(torch.cat((h_next_e, h_source))) #а здесь не смещаить h_source?\n",
    "# #         y_target = self.choice(z_target, h_target)\n",
    "        \n",
    "#         return h_next_e\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model = SRAN(conf)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "itt = iter(train_gen)\n",
    "vi, e1, graph = next(itt)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "vi, e1, graph"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(tensor([  0,   1,   2,  ..., 898, 937, 938]),\n",
       " tensor([[  33,   59,   69,  ..., 1927, 1929, 1931],\n",
       "         [  34,   61,   71,  ..., 1799, 1833, 1867]]),\n",
       " tensor([[  32,   11,   22,  ..., 1867, 1930, 1962],\n",
       "         [   0,    0,    0,  ..., 1963, 1963, 1963]]))"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "model(vi, dset.features, graph, e1)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[ 0.0284,  0.1295, -0.1574,  ...,  0.1038,  0.1757, -0.0239],\n",
       "         [-0.1961, -0.0377, -0.3235,  ..., -0.3184, -0.0484, -0.0732],\n",
       "         [ 0.3629, -0.0362,  0.0892,  ..., -0.1354, -0.1102, -0.0791],\n",
       "         ...,\n",
       "         [-0.1120, -0.3503, -0.0158,  ...,  0.0295,  0.0962,  0.2008],\n",
       "         [-0.2932,  0.0186, -0.0791,  ..., -0.0857, -0.1671,  0.0564],\n",
       "         [-0.0409, -0.1109,  0.0575,  ...,  0.1184,  0.0375, -0.0748]]],\n",
       "       grad_fn=<UnsafeViewBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "dset.features"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        [1., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 0., 1.]])"
      ]
     },
     "metadata": {},
     "execution_count": 73
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "source": [],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[ 0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,\n",
      "          1.0000e+00,  0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00,\n",
      "          0.0000e+00,  1.0000e+00,  0.0000e+00,  1.0000e+00],\n",
      "        [ 8.4147e-01,  5.4030e-01,  6.8156e-01,  7.3176e-01,  5.3317e-01,\n",
      "          8.4601e-01,  4.0931e-01,  9.1240e-01,  3.1098e-01,  9.5042e-01,\n",
      "          2.3492e-01,  9.7201e-01,  1.7689e-01,  9.8423e-01,  1.3296e-01,\n",
      "          9.9112e-01,  9.9833e-02,  9.9500e-01,  7.4919e-02,  9.9719e-01,\n",
      "          5.6204e-02,  9.9842e-01,  4.2157e-02,  9.9911e-01,  3.1618e-02,\n",
      "          9.9950e-01,  2.3712e-02,  9.9972e-01,  1.7782e-02,  9.9984e-01,\n",
      "          1.3335e-02,  9.9991e-01,  9.9998e-03,  9.9995e-01,  7.4989e-03,\n",
      "          9.9997e-01,  5.6234e-03,  9.9998e-01,  4.2170e-03,  9.9999e-01,\n",
      "          3.1623e-03,  9.9999e-01,  2.3714e-03,  1.0000e+00,  1.7783e-03,\n",
      "          1.0000e+00,  1.3335e-03,  1.0000e+00,  1.0000e-03,  1.0000e+00,\n",
      "          7.4989e-04,  1.0000e+00,  5.6234e-04,  1.0000e+00,  4.2170e-04,\n",
      "          1.0000e+00,  3.1623e-04,  1.0000e+00,  2.3714e-04,  1.0000e+00,\n",
      "          1.7783e-04,  1.0000e+00,  1.3335e-04,  1.0000e+00],\n",
      "        [ 9.0930e-01, -4.1615e-01,  9.9748e-01,  7.0948e-02,  9.0213e-01,\n",
      "          4.3146e-01,  7.4690e-01,  6.6493e-01,  5.9113e-01,  8.0658e-01,\n",
      "          4.5669e-01,  8.8962e-01,  3.4821e-01,  9.3742e-01,  2.6355e-01,\n",
      "          9.6464e-01,  1.9867e-01,  9.8007e-01,  1.4942e-01,  9.8877e-01,\n",
      "          1.1223e-01,  9.9368e-01,  8.4239e-02,  9.9645e-01,  6.3203e-02,\n",
      "          9.9800e-01,  4.7410e-02,  9.9888e-01,  3.5558e-02,  9.9937e-01,\n",
      "          2.6667e-02,  9.9964e-01,  1.9999e-02,  9.9980e-01,  1.4997e-02,\n",
      "          9.9989e-01,  1.1247e-02,  9.9994e-01,  8.4338e-03,  9.9996e-01,\n",
      "          6.3245e-03,  9.9998e-01,  4.7427e-03,  9.9999e-01,  3.5566e-03,\n",
      "          9.9999e-01,  2.6670e-03,  1.0000e+00,  2.0000e-03,  1.0000e+00,\n",
      "          1.4998e-03,  1.0000e+00,  1.1247e-03,  1.0000e+00,  8.4339e-04,\n",
      "          1.0000e+00,  6.3246e-04,  1.0000e+00,  4.7427e-04,  1.0000e+00,\n",
      "          3.5566e-04,  1.0000e+00,  2.6670e-04,  1.0000e+00],\n",
      "        [ 1.4112e-01, -9.8999e-01,  7.7827e-01, -6.2793e-01,  9.9325e-01,\n",
      "         -1.1597e-01,  9.5363e-01,  3.0097e-01,  8.1265e-01,  5.8275e-01,\n",
      "          6.5290e-01,  7.5744e-01,  5.0854e-01,  8.6104e-01,  3.8947e-01,\n",
      "          9.2104e-01,  2.9552e-01,  9.5534e-01,  2.2308e-01,  9.7480e-01,\n",
      "          1.6790e-01,  9.8580e-01,  1.2617e-01,  9.9201e-01,  9.4726e-02,\n",
      "          9.9550e-01,  7.1081e-02,  9.9747e-01,  5.3323e-02,  9.9858e-01,\n",
      "          3.9995e-02,  9.9920e-01,  2.9995e-02,  9.9955e-01,  2.2495e-02,\n",
      "          9.9975e-01,  1.6869e-02,  9.9986e-01,  1.2651e-02,  9.9992e-01,\n",
      "          9.4867e-03,  9.9995e-01,  7.1141e-03,  9.9997e-01,  5.3348e-03,\n",
      "          9.9999e-01,  4.0006e-03,  9.9999e-01,  3.0000e-03,  1.0000e+00,\n",
      "          2.2497e-03,  1.0000e+00,  1.6870e-03,  1.0000e+00,  1.2651e-03,\n",
      "          1.0000e+00,  9.4868e-04,  1.0000e+00,  7.1141e-04,  1.0000e+00,\n",
      "          5.3348e-04,  1.0000e+00,  4.0006e-04,  1.0000e+00],\n",
      "        [-7.5680e-01, -6.5364e-01,  1.4154e-01, -9.8993e-01,  7.7847e-01,\n",
      "         -6.2768e-01,  9.9328e-01, -1.1573e-01,  9.5358e-01,  3.0114e-01,\n",
      "          8.1257e-01,  5.8286e-01,  6.5283e-01,  7.5751e-01,  5.0847e-01,\n",
      "          8.6108e-01,  3.8942e-01,  9.2106e-01,  2.9548e-01,  9.5535e-01,\n",
      "          2.2304e-01,  9.7481e-01,  1.6788e-01,  9.8581e-01,  1.2615e-01,\n",
      "          9.9201e-01,  9.4713e-02,  9.9550e-01,  7.1071e-02,  9.9747e-01,\n",
      "          5.3316e-02,  9.9858e-01,  3.9989e-02,  9.9920e-01,  2.9991e-02,\n",
      "          9.9955e-01,  2.2492e-02,  9.9975e-01,  1.6867e-02,  9.9986e-01,\n",
      "          1.2649e-02,  9.9992e-01,  9.4854e-03,  9.9995e-01,  7.1131e-03,\n",
      "          9.9997e-01,  5.3341e-03,  9.9999e-01,  4.0000e-03,  9.9999e-01,\n",
      "          2.9996e-03,  1.0000e+00,  2.2494e-03,  1.0000e+00,  1.6868e-03,\n",
      "          1.0000e+00,  1.2649e-03,  1.0000e+00,  9.4855e-04,  1.0000e+00,\n",
      "          7.1131e-04,  1.0000e+00,  5.3341e-04,  1.0000e+00],\n",
      "        [-9.5892e-01,  2.8366e-01, -5.7113e-01, -8.2086e-01,  3.2394e-01,\n",
      "         -9.4608e-01,  8.5890e-01, -5.1215e-01,  9.9995e-01, -1.0342e-02,\n",
      "          9.2676e-01,  3.7566e-01,  7.7653e-01,  6.3008e-01,  6.1844e-01,\n",
      "          7.8583e-01,  4.7943e-01,  8.7758e-01,  3.6622e-01,  9.3053e-01,\n",
      "          2.7748e-01,  9.6073e-01,  2.0929e-01,  9.7785e-01,  1.5746e-01,\n",
      "          9.8753e-01,  1.1829e-01,  9.9298e-01,  8.8797e-02,  9.9605e-01,\n",
      "          6.6627e-02,  9.9778e-01,  4.9979e-02,  9.9875e-01,  3.7486e-02,\n",
      "          9.9930e-01,  2.8113e-02,  9.9960e-01,  2.1083e-02,  9.9978e-01,\n",
      "          1.5811e-02,  9.9988e-01,  1.1857e-02,  9.9993e-01,  8.8913e-03,\n",
      "          9.9996e-01,  6.6676e-03,  9.9998e-01,  5.0000e-03,  9.9999e-01,\n",
      "          3.7495e-03,  9.9999e-01,  2.8117e-03,  1.0000e+00,  2.1085e-03,\n",
      "          1.0000e+00,  1.5811e-03,  1.0000e+00,  1.1857e-03,  1.0000e+00,\n",
      "          8.8914e-04,  1.0000e+00,  6.6676e-04,  1.0000e+00],\n",
      "        [-2.7942e-01,  9.6017e-01, -9.7740e-01, -2.1142e-01, -2.3037e-01,\n",
      "         -9.7310e-01,  5.7403e-01, -8.1884e-01,  9.4715e-01, -3.2080e-01,\n",
      "          9.8907e-01,  1.4743e-01,  8.7574e-01,  4.8278e-01,  7.1743e-01,\n",
      "          6.9663e-01,  5.6464e-01,  8.2534e-01,  4.3491e-01,  9.0047e-01,\n",
      "          3.3104e-01,  9.4362e-01,  2.5033e-01,  9.6816e-01,  1.8860e-01,\n",
      "          9.8205e-01,  1.4180e-01,  9.8989e-01,  1.0649e-01,  9.9431e-01,\n",
      "          7.9926e-02,  9.9680e-01,  5.9964e-02,  9.9820e-01,  4.4978e-02,\n",
      "          9.9899e-01,  3.3734e-02,  9.9943e-01,  2.5299e-02,  9.9968e-01,\n",
      "          1.8973e-02,  9.9982e-01,  1.4228e-02,  9.9990e-01,  1.0669e-02,\n",
      "          9.9994e-01,  8.0010e-03,  9.9997e-01,  6.0000e-03,  9.9998e-01,\n",
      "          4.4993e-03,  9.9999e-01,  3.3740e-03,  9.9999e-01,  2.5302e-03,\n",
      "          1.0000e+00,  1.8974e-03,  1.0000e+00,  1.4228e-03,  1.0000e+00,\n",
      "          1.0670e-03,  1.0000e+00,  8.0011e-04,  1.0000e+00],\n",
      "        [ 6.5699e-01,  7.5390e-01, -8.5931e-01,  5.1145e-01, -7.1372e-01,\n",
      "         -7.0043e-01,  1.8858e-01, -9.8206e-01,  8.0042e-01, -5.9944e-01,\n",
      "          9.9603e-01, -8.9047e-02,  9.4733e-01,  3.2026e-01,  8.0369e-01,\n",
      "          5.9505e-01,  6.4422e-01,  7.6484e-01,  5.0115e-01,  8.6536e-01,\n",
      "          3.8355e-01,  9.2352e-01,  2.9092e-01,  9.5675e-01,  2.1956e-01,\n",
      "          9.7560e-01,  1.6523e-01,  9.8625e-01,  1.2416e-01,  9.9226e-01,\n",
      "          9.3211e-02,  9.9565e-01,  6.9943e-02,  9.9755e-01,  5.2468e-02,\n",
      "          9.9862e-01,  3.9354e-02,  9.9923e-01,  2.9514e-02,  9.9956e-01,\n",
      "          2.2134e-02,  9.9976e-01,  1.6599e-02,  9.9986e-01,  1.2448e-02,\n",
      "          9.9992e-01,  9.3345e-03,  9.9996e-01,  6.9999e-03,  9.9998e-01,\n",
      "          5.2492e-03,  9.9999e-01,  3.9364e-03,  9.9999e-01,  2.9519e-03,\n",
      "          1.0000e+00,  2.2136e-03,  1.0000e+00,  1.6600e-03,  1.0000e+00,\n",
      "          1.2448e-03,  1.0000e+00,  9.3346e-04,  1.0000e+00],\n",
      "        [ 9.8936e-01, -1.4550e-01, -2.8023e-01,  9.5993e-01, -9.7726e-01,\n",
      "         -2.1204e-01, -2.2990e-01, -9.7321e-01,  5.7432e-01, -8.1863e-01,\n",
      "          9.4723e-01, -3.2054e-01,  9.8904e-01,  1.4763e-01,  8.7567e-01,\n",
      "          4.8291e-01,  7.1736e-01,  6.9671e-01,  5.6457e-01,  8.2538e-01,\n",
      "          4.3485e-01,  9.0050e-01,  3.3099e-01,  9.4363e-01,  2.5029e-01,\n",
      "          9.6817e-01,  1.8857e-01,  9.8206e-01,  1.4178e-01,  9.8990e-01,\n",
      "          1.0648e-01,  9.9431e-01,  7.9915e-02,  9.9680e-01,  5.9956e-02,\n",
      "          9.9820e-01,  4.4972e-02,  9.9899e-01,  3.3729e-02,  9.9943e-01,\n",
      "          2.5296e-02,  9.9968e-01,  1.8970e-02,  9.9982e-01,  1.4226e-02,\n",
      "          9.9990e-01,  1.0668e-02,  9.9994e-01,  7.9999e-03,  9.9997e-01,\n",
      "          5.9991e-03,  9.9998e-01,  4.4987e-03,  9.9999e-01,  3.3736e-03,\n",
      "          9.9999e-01,  2.5298e-03,  1.0000e+00,  1.8971e-03,  1.0000e+00,\n",
      "          1.4226e-03,  1.0000e+00,  1.0668e-03,  1.0000e+00],\n",
      "        [ 4.1212e-01, -9.1113e-01,  4.4919e-01,  8.9343e-01, -9.3982e-01,\n",
      "          3.4166e-01, -6.0811e-01, -7.9385e-01,  2.9126e-01, -9.5664e-01,\n",
      "          8.4542e-01, -5.3410e-01,  9.9956e-01, -2.9651e-02,  9.3210e-01,\n",
      "          3.6220e-01,  7.8333e-01,  6.2161e-01,  6.2482e-01,  7.8077e-01,\n",
      "          4.8478e-01,  8.7464e-01,  3.7048e-01,  9.2884e-01,  2.8078e-01,\n",
      "          9.5977e-01,  2.1181e-01,  9.7731e-01,  1.5936e-01,  9.8722e-01,\n",
      "          1.1973e-01,  9.9281e-01,  8.9879e-02,  9.9595e-01,  6.7439e-02,\n",
      "          9.9772e-01,  5.0589e-02,  9.9872e-01,  3.7944e-02,  9.9928e-01,\n",
      "          2.8457e-02,  9.9960e-01,  2.1341e-02,  9.9977e-01,  1.6004e-02,\n",
      "          9.9987e-01,  1.2001e-02,  9.9993e-01,  8.9999e-03,  9.9996e-01,\n",
      "          6.7490e-03,  9.9998e-01,  5.0611e-03,  9.9999e-01,  3.7953e-03,\n",
      "          9.9999e-01,  2.8460e-03,  1.0000e+00,  2.1342e-03,  1.0000e+00,\n",
      "          1.6005e-03,  1.0000e+00,  1.2002e-03,  1.0000e+00]])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# eshe ne pravil!!!!\n",
    "\n",
    "def train(sampler, mod, optimizer, data, bsz):\n",
    "    mod.train()\n",
    "        \n",
    "    loss = 0   \n",
    "    encoded_features = mod.encode(data.x.to(device = mod.device))\n",
    "\n",
    "    ii=0\n",
    "    for inp in sampler(data, batch_size = bsz):\n",
    "        ii +=1\n",
    "        encoded_features = mod.encode(data.x.to(device = mod.device))\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        pred_e, targ_e, w0 = mod.iterate(data, inp, encoded, bsz) \n",
    "#         n_1.append(targ_e.sum())\n",
    "#         num_e.append(pred_e.shape[0])\n",
    "        if mod.label_smooth:\n",
    "            loss += cal_edge_loss(pred_e, targ_e)\n",
    "        else:\n",
    "            loss += F.binary_cross_entropy_with_logits(pred_e, targ_e)    \n",
    "#             loss += F.binary_cross_entropy_with_logits(pred_e, targ_e, \n",
    "#                                                        pos_weight = torch.tensor([w0], device = mod.device))    \n",
    "            \n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "#     d_train\n",
    "    train(data_gen_edges_dyn, model, optimizer, data, B_SIZE)\n",
    "#     train(data_gen_edges_dyn, model, optimizer, d_train, B_SIZE)\n",
    "#     train(data_gen_edges_dyn, model, optimizer, data, B_SIZE)\n",
    "#     acc_e_test = test( data_gen_edges_dyn, model, d_test, 1)\n",
    "#     acc_e_test = test( data_gen_edges_dyn, model, data, 1)\n",
    "#     acc = acc_e_test.mean()\n",
    "#     print(acc, np.median(acc_e_test))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# eshe ne pravil!!!!\n",
    "@torch.no_grad()\n",
    "def test(sampler,mod, data, bsz, return_y = False, dtype = 'test'):\n",
    "    mod.eval()\n",
    "        \n",
    "    out = []\n",
    "    n_1, acc_e,num_e = [],[],[]\n",
    "    # init_state = torch.zeros((data.num_nodes*bsz,  mod.nd), device=mod.device)\n",
    "    encoded = mod.encode(data.x.to(device = mod.device))\n",
    "    indd = torch.cat(bsz*[torch.arange(data.num_nodes, device = mod.device)])\n",
    "    mod.reset_state(encoded[indd,:])\n",
    "    for inp in sampler(data, batch_size = bsz, step_max = 512, dtype = dtype):\n",
    "        pred_e, targ_e, w0 = mod.iterate(data, inp, encoded, bsz) \n",
    "        if return_y:\n",
    "            out.append((pred_e, targ_e, w0))#, dim=1\n",
    "        pred_e = torch.round(torch.sigmoid(pred_e))\n",
    "#         print('pred_e', pred_e)\n",
    "        tp_e = pred_e.eq(targ_e).sum().item()\n",
    "        n_e = pred_e.shape[0]\n",
    "        \n",
    "        n_1.append(targ_e.sum())\n",
    "\n",
    "        acc_e.append(tp_e)\n",
    "        num_e.append(n_e)\n",
    "        \n",
    "    if return_y:\n",
    "        return out\n",
    "    else:\n",
    "        return  np.array(acc_e)/(np.array(num_e)+1e-9)\n",
    "\n",
    "    "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}