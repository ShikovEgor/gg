{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "from typing import Union, Tuple\n",
    "from typing import List, Optional, Set, get_type_hints\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.typing import OptPairTensor, Adj, Size\n",
    "from torch_scatter import gather_csr, scatter, segment_csr\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Sequential, ReLU, Linear, Dropout, BatchNorm1d\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from src.data_preparation import RWDataset, data_gen_e_aug\n",
    "from src.models import GraphNet, add_weight_decay\n",
    "from src.utils import create_nx_graph,create_gt_graph, draw_deg_distr, relabel, init_graph\n",
    "from src.utils import sel_start_node, sel_start_node_old, get_errors\n",
    "from src.train import LabelSmoothing\n",
    "from src.utils import NodeSelector\n",
    "from src.model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSIZE = 16\n",
    "dset = RWDataset('')\n",
    "train_loader = DataLoader(dset, batch_size=BSIZE, shuffle=False)# , exclude_keys=['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = data_gen_e_aug(train_loader, dset.slices['edge_index'], batch_size = BSIZE,step = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "itt = iter(train_gen)\n",
    "vi, ei = next(itt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,   54,    0,  ..., 1835, 2790, 1967],\n",
       "        [   0,    0,    2,  ..., 4343, 4343, 4343]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_features, arch, device):\n",
    "    dc = ModelGen(num_features, device, arch)\n",
    "    dc = dc.to(device = device)\n",
    "    prs = add_weight_decay(dc.named_parameters(), arch['weight_decay'])\n",
    "#     optimizer = RAdam(prs, lr=arch['lr'])\n",
    "    optimizer = optim.Adam(prs, lr=arch['lr'])  \n",
    "    return dc, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4344, 128])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(vi, dset.features, ei).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "arch0 = {'input_dim': dset.features.shape[1], \n",
    "        'hidden_dim': 32, \n",
    "        'block_size': None,\n",
    "        'num_layers_tr': 2,\n",
    "        'num_heads': 8,\n",
    "        'attn_pdrop': 0.5,\n",
    "        'resid_pdrop': 0.5,\n",
    "        'embd_pdrop': 0.5,\n",
    "        'gnn_pdrop': 0.5,\n",
    "        'num_gnn_layers': 1,\n",
    "        'mlp_pdrop': 0.5}\n",
    "conf = objectview(arch0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureTransform(torch.nn.Module):\n",
    "    def __init__(self, num_features, nn1 , nn2 = None):\n",
    "        super().__init__()      \n",
    "        lin1 = nn.Linear(num_features,nn1)\n",
    "        lin1.bias.data.fill_(0.)  \n",
    "        bn1 = BatchNorm1d(nn1)\n",
    "        self.encoder = Sequential(lin1, ReLU(),bn1)\n",
    "        self.dropout = Dropout(0.2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         self.dropout(x)\n",
    "        z = self.encoder(x)\n",
    "        return z\n",
    "\n",
    "class ModelGen(nn.Module):\n",
    "    def __init__(self, num_features, device, arch):\n",
    "        super(ModelGen, self).__init__()\n",
    "        self.lin_inp = FeatureTransform(num_features, arch['nd'])\n",
    "\n",
    "        self.n_layers_gnn = arch['nlayers_gnn']\n",
    "        self.convs = nn.ModuleList()\n",
    "        for i in range(self.n_layers_gnn):\n",
    "            self.convs.append(SAGEConv(arch['nd'], arch['nd']))\n",
    "        self.batch_norms = torch.nn.ModuleList(\n",
    "            [nn.BatchNorm1d(\n",
    "                num_features=dims[i+1]\n",
    "            ) for i in range(num_layers-1)])\n",
    "        \n",
    "    def forward(self, v_ind, features, edge_index):\n",
    "        features = self.lin_inp(features)\n",
    "        x = features[v_ind]\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != self.n_layers_gnn - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "    \n",
    "class GraphNet(nn.Module):\n",
    "    def __init__(self, input_dim,\n",
    "                       hidden_dim,\n",
    "                       output_dim,\n",
    "                       num_layers,\n",
    "                       dropout_p):\n",
    "\n",
    "        super(GraphNet, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        dims = [input_dim] + [hidden_dim] * (num_layers - 1) + [output_dim]\n",
    "        self.convs = torch.nn.ModuleList(\n",
    "            [SAGEConv(\n",
    "                in_channels=dims[i],\n",
    "                out_channels=dims[i+1]\n",
    "            ) for i in range(num_layers)]\n",
    "        )\n",
    "        self.batch_norms = torch.nn.ModuleList(\n",
    "            [torch.nn.BatchNorm1d(\n",
    "                num_features=dims[i+1]\n",
    "            ) for i in range(num_layers-1)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i in range(self.num_layers-1):\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.batch_norms[i](x)\n",
    "            x = F.gelu(x)\n",
    "            # x = F.dropout(x, p=self.dropout_p, training=self.training)\n",
    "\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class SRAN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SRAN, self).__init__()\n",
    "\n",
    "        self.input_dim = config.input_dim\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.block_size = config.block_size\n",
    "        self.num_layers = config.num_layers\n",
    "        self.num_heads = config.num_heads\n",
    "        self.attn_pdrop = config.attn_pdrop\n",
    "        self.resid_pdrop = config.resid_pdrop\n",
    "        self.embd_pdrop = config.embd_pdrop\n",
    "        self.gnn_pdrop  = config.gnn_pdrop\n",
    "        self.num_gnn_layers = config.num_gnn_layers\n",
    "        self.mlp_pdrop = config.mlp_pdrop\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "            nn.Dropout(self.mlp_pdrop)\n",
    "        )\n",
    "\n",
    "        self.gnn = GCN(\n",
    "            input_dim=self.input_dim,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            output_dim=self.hidden_dim,\n",
    "            num_layers=self.num_gnn_layers,\n",
    "            dropout_p=self.gnn_pdrop\n",
    "        )\n",
    "\n",
    "        self.gpt = GPT(\n",
    "            block_size=self.block_size,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            num_heads=self.num_heads,\n",
    "            attn_pdrop=self.attn_pdrop,\n",
    "            resid_pdrop=self.resid_pdrop,\n",
    "            embd_pdrop=self.embd_pdrop\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def _infer(self, x, edge_index):\n",
    "        node_feat = self.mlp(x)\n",
    "\n",
    "        # if there is no edges yet -- send the first n nodes double\n",
    "        # embeddings to gpt (n - block size of gpt)\n",
    "#         if edge_index is None:\n",
    "#             gpt_input = \n",
    "\n",
    "        node_feat = self.gnn(node_feat, edge_index)\n",
    "\n",
    "    def _sample(self):\n",
    "        pass\n",
    "    \n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'a': 1, 'b': 2}\n",
    "o = objectview(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = torch.device('cuda:1')\n",
    "arch0 = {'nd': 128, 'weight_decay': 0.0007,'lr': 0.001,'nlayers_gnn': 2,'add_trans': False, 'conv_type':'gin', #'gin'\n",
    "        'node_agg_type': 'mean','agg_type': 'sum_gate','fin_type_e': 'l2_sum',\n",
    "        'lb_smth': False}\n",
    "# diff_mlp\n",
    "# diff_het\n",
    "# het\n",
    "model, optimizer = create_model(dset.features.shape[1], arch0, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyConv(MessagePassing):\n",
    "    def __init__(self, **kwargs):  # yapf: disable\n",
    "        kwargs.setdefault('aggr', 'add')\n",
    "        super(MyConv, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, x: Union[Tensor, OptPairTensor], edge_index: Adj, deg: Tensor,\n",
    "                size: Size = None) -> Tensor:\n",
    "        \n",
    "        x = torch.cat((x, deg.view(-1,1)), dim=1)\n",
    "        if isinstance(x, Tensor):\n",
    "            x: OptPairTensor = (x, x)      \n",
    "        out = self.propagate(edge_index, x=x, size=size)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j: Tensor) -> Tensor:\n",
    "        return x_j\n",
    "    \n",
    "    def aggregate(self, inputs: Tensor, index: Tensor,\n",
    "                  ptr: Optional[Tensor] = None,\n",
    "                  dim_size: Optional[int] = None) -> Tensor:\n",
    "        r\"\"\"Aggregates messages from neighbors as\n",
    "        :math:`\\square_{j \\in \\mathcal{N}(i)}`.\n",
    "\n",
    "        Takes in the output of message computation as first argument and any\n",
    "        argument which was initially passed to :meth:`propagate`.\n",
    "\n",
    "        By default, this function will delegate its call to scatter functions\n",
    "        that support \"add\", \"mean\" and \"max\" operations as specified in\n",
    "        :meth:`__init__` by the :obj:`aggr` argument.\n",
    "        \"\"\"\n",
    "        if ptr is not None:\n",
    "            ptr = expand_left(ptr, dim=self.node_dim, dims=inputs.dim())\n",
    "            return segment_csr(inputs, ptr, reduce=self.aggr)\n",
    "        else:\n",
    "            print(inputs, index)\n",
    "            return scatter(inputs, index, dim=self.node_dim, dim_size=dim_size,\n",
    "                           reduce=self.aggr)\n",
    "        \n",
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "        self.lin = torch.nn.Linear(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # x has shape [N, in_channels]\n",
    "        # edge_index has shape [2, E]\n",
    "\n",
    "        # Step 1: Add self-loops to the adjacency matrix.\n",
    "        edge_index, _ = add_self_loops(edge_index, num_nodes=x.size(0))\n",
    "\n",
    "        # Step 2: Linearly transform node feature matrix.\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # Step 3: Compute normalization.\n",
    "        row, col = edge_index\n",
    "        deg = degree(col, x.size(0), dtype=x.dtype)\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        # Step 4-5: Start propagating messages.\n",
    "        return self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        # x_j has shape [E, out_channels]\n",
    "\n",
    "        # Step 4: Normalize node features.\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GENConv(MessagePassing):\n",
    "    def __init__(self):\n",
    "        kwargs.setdefault('aggr', None)\n",
    "        super(GENConv, self).__init__(**kwargs)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        out = self.propagate(edge_index, x=x, size=size)\n",
    "        if self.msg_norm is not None:\n",
    "            out = self.msg_norm(x[0], out)\n",
    "        x_r = x[1]\n",
    "        if x_r is not None:\n",
    "            out += x_r\n",
    "        return self.mlp(out)\n",
    "\n",
    "    def message(self, x_j: Tensor, edge_attr: OptTensor) -> Tensor:\n",
    "        return F.relu(msg) + self.eps\n",
    "\n",
    "    def aggregate(self, inputs: Tensor, index: Tensor,\n",
    "                  dim_size: Optional[int] = None) -> Tensor:\n",
    "\n",
    "        out = scatter_softmax(inputs * self.t, index, dim=self.node_dim)\n",
    "        return scatter(inputs * out, index, dim=self.node_dim,\n",
    "                           dim_size=dim_size, reduce='sum')\n",
    "\n",
    "#         elif self.aggr == 'softmax_sg':\n",
    "#             out = scatter_softmax(inputs * self.t, index,\n",
    "#                                   dim=self.node_dim).detach()\n",
    "#             return scatter(inputs * out, index, dim=self.node_dim,\n",
    "#                            dim_size=dim_size, reduce='sum')\n",
    "\n",
    "#         else:\n",
    "#             min_value, max_value = 1e-7, 1e1\n",
    "#             torch.clamp_(inputs, min_value, max_value)\n",
    "#             out = scatter(torch.pow(inputs, self.p), index, dim=self.node_dim,\n",
    "#                           dim_size=dim_size, reduce='mean')\n",
    "#             torch.clamp_(out, min_value, max_value)\n",
    "#             return torch.pow(out, 1 / self.p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = MyConv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "el = [(0,13),(0,5),(0,14),(13,1),(0,18),(0,8),(14,7),(0,11),(0,3),\n",
    "         (3,10),(0,19),(11,4),(1,9),(9,12),(0,12),(3,7),(0,16),(11,8),\n",
    "         (1,3),(1,6),(0,13),(14,13),(11,15),(0,12),(4,17),(11,12),(3,2)]\n",
    "ei = torch.tensor(el).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_dict = defaultdict(set)\n",
    "for e in ei.T:\n",
    "    e = e.to(dtype=torch.long)\n",
    "    ei_dict[e[0]].add(e[1]) \n",
    "    ei_dict[e[1]].add(e[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{tensor(0): {tensor(13)}}"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = {torch.tensor(0): {torch.tensor(13)}}\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = F.one_hot(ei[0],num_classes=20)+F.one_hot(ei[1],num_classes=20)\n",
    "oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cumsum(oh, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "row, col = ei\n",
    "adj_t = SparseTensor(row=col, col=row,\n",
    "                     sparse_sizes=(data.num_nodes, data.num_nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensor(row=tensor([ 1,  2,  3,  3,  4,  5,  6,  7,  7,  8,  8,  9, 10, 11, 12, 12, 12, 12,\n",
       "                           13, 13, 13, 14, 15, 16, 17, 18, 19]),\n",
       "             col=tensor([13,  3,  0,  1, 11,  0,  1,  3, 14,  0, 11,  1,  3,  0,  0,  0,  9, 11,\n",
       "                            0,  0, 14,  0, 11,  0,  4,  0,  0]),\n",
       "             size=(275, 275), nnz=27, density=0.04%)"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseTensor(row=tensor([0, 0]),\n",
       "             col=tensor([0, 1]),\n",
       "             size=(1, 275), nnz=2, density=0.73%)"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "adj_t[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "delta_v = torch.ones(10, 8)\n",
    "delta_v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.zeros(16, 8, dtype=src.dtype).scatter_add_(0, index, src)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12,  4,  1,  5,  2,  1,  1,  2,  2,  2,  1,  5,  4,  4,  3,  1,  1,  1,\n",
       "         1,  1])"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_, dg = torch.unique(ei, return_counts = True)\n",
    "dg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3218, 0.3033, 0.4526, 0.0737, 0.3362, 0.4346, 0.5787, 0.9123, 1.0000],\n",
      "        [0.3218, 0.3033, 0.4526, 0.0737, 0.3362, 0.4346, 0.5787, 0.9123, 1.0000],\n",
      "        [0.3218, 0.3033, 0.4526, 0.0737, 0.3362, 0.4346, 0.5787, 0.9123, 1.0000],\n",
      "        [0.9967, 0.6499, 0.4298, 0.2115, 0.7027, 0.8352, 0.6765, 0.0435, 1.0000],\n",
      "        [0.3218, 0.3033, 0.4526, 0.0737, 0.3362, 0.4346, 0.5787, 0.9123, 1.0000],\n",
      "        [0.3218, 0.3033, 0.4526, 0.0737, 0.3362, 0.4346, 0.5787, 0.9123, 1.0000],\n",
      "        [0.7772, 0.1651, 0.0392, 0.4880, 0.2471, 0.5597, 0.5317, 0.4280, 1.0000],\n",
      "        [0.3218, 0.3033, 0.4526, 0.0737, 0.3362, 0.4346, 0.5787, 0.9123, 1.0000],\n",
      "        [0.3218, 0.3033, 0.4526, 0.0737, 0.3362, 0.4346, 0.5787, 0.9123, 1.0000],\n",
      "        [0.0049, 0.0289, 0.7177, 0.0580, 0.8168, 0.5887, 0.8685, 0.2321, 1.0000],\n",
      "        [0.3218, 0.3033, 0.4526, 0.0737, 0.3362, 0.4346, 0.5787, 0.9123, 1.0000],\n",
      "        [0.7230, 0.3680, 0.4621, 0.3198, 0.0290, 0.2466, 0.4086, 0.0826, 1.0000],\n",
      "        [0.7480, 0.2289, 0.7284, 0.9652, 0.4299, 0.1629, 0.0740, 0.6505, 1.0000],\n",
      "        [0.8849, 0.0888, 0.0959, 0.5236, 0.5322, 0.9872, 0.6792, 0.0489, 1.0000],\n",
      "        [0.3218, 0.3033, 0.4526, 0.0737, 0.3362, 0.4346, 0.5787, 0.9123, 1.0000],\n",
      "        [0.0049, 0.0289, 0.7177, 0.0580, 0.8168, 0.5887, 0.8685, 0.2321, 1.0000],\n",
      "        [0.3218, 0.3033, 0.4526, 0.0737, 0.3362, 0.4346, 0.5787, 0.9123, 1.0000],\n",
      "        [0.7230, 0.3680, 0.4621, 0.3198, 0.0290, 0.2466, 0.4086, 0.0826, 1.0000],\n",
      "        [0.7480, 0.2289, 0.7284, 0.9652, 0.4299, 0.1629, 0.0740, 0.6505, 1.0000],\n",
      "        [0.7480, 0.2289, 0.7284, 0.9652, 0.4299, 0.1629, 0.0740, 0.6505, 1.0000],\n",
      "        [0.3218, 0.3033, 0.4526, 0.0737, 0.3362, 0.4346, 0.5787, 0.9123, 1.0000],\n",
      "        [0.7772, 0.1651, 0.0392, 0.4880, 0.2471, 0.5597, 0.5317, 0.4280, 1.0000],\n",
      "        [0.7230, 0.3680, 0.4621, 0.3198, 0.0290, 0.2466, 0.4086, 0.0826, 1.0000],\n",
      "        [0.3218, 0.3033, 0.4526, 0.0737, 0.3362, 0.4346, 0.5787, 0.9123, 1.0000],\n",
      "        [0.0780, 0.0532, 0.1328, 0.1744, 0.7345, 0.7418, 0.1649, 0.2961, 1.0000],\n",
      "        [0.7230, 0.3680, 0.4621, 0.3198, 0.0290, 0.2466, 0.4086, 0.0826, 1.0000],\n",
      "        [0.0049, 0.0289, 0.7177, 0.0580, 0.8168, 0.5887, 0.8685, 0.2321, 1.0000]]) tensor([13,  5, 14,  1, 18,  8,  7, 11,  3, 10, 19,  4,  9, 12, 12,  7, 16,  8,\n",
      "         3,  6, 13, 13, 15, 12, 17, 12,  2])\n"
     ]
    }
   ],
   "source": [
    "xx = torch.rand(20,8)\n",
    "y = conv(x = xx, edge_index = ei, deg = torch.ones(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genv",
   "language": "python",
   "name": "genv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
