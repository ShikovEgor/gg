{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os.path as osp\n",
    "import pathlib\n",
    "\n",
    "from typing import Union, Tuple\n",
    "from typing import List, Optional, Set, get_type_hints\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.data import InMemoryDataset\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.utils import to_undirected\n",
    "from torch_geometric.typing import OptPairTensor, Adj, Size\n",
    "from torch_scatter import gather_csr, scatter, segment_csr\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Sequential, ReLU, Linear, Dropout, BatchNorm1d\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# from src.data_preparation import RWDataset, data_gen_e_aug\n",
    "# from src.models import GraphNet, add_weight_decay\n",
    "from src.utils import create_nx_graph,create_gt_graph, draw_deg_distr, relabel, init_graph\n",
    "from src.utils import sel_start_node, sel_start_node_old, get_errors\n",
    "from src.train import LabelSmoothing\n",
    "from src.utils import NodeSelector\n",
    "from src.model import GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_PYG_datasets(d_name = 'PROTEINS'):\n",
    "    pth = '/data/egor/graph_generation/seq/gg/data'\n",
    "    path = osp.join(pathlib.Path().absolute(), pth , d_name)\n",
    "    dset = TUDataset(path, d_name)\n",
    "    \n",
    "    #sozdal otdelno pole features, a v pole x pomestil indexi - t.k. vektora vershin u nas menyayutsya\n",
    "    dset.features = dset.data.x \n",
    "    dset.data.x = torch.arange(dset.data.x.shape[0])\n",
    "    return dset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = load_PYG_datasets('PROTEINS')\n",
    "#dset = RWDataset('')  random walks for Cora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSIZE = 16\n",
    "train_loader = DataLoader(dset, batch_size=BSIZE, shuffle=False)# , exclude_keys=['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_gen_e_aug(train_loader, slices, batch_size = 4, step = 1):\n",
    "    r\"\"\" berem skleennie graphi, iteriruem po vershinam, stroim augmented graph\n",
    "\n",
    "    Args:\n",
    "        train_loader: PyG DataLoader\n",
    "        slices: indexy reber, po kotorim skleivali nabor graphov\n",
    "        batch_size:\n",
    "        step: po skolko reber narashivaem graph, obichno 1\n",
    "    Outputs:\n",
    "        vi: indexi vershin v ishodnom graphe\n",
    "        graph: augmented graph\n",
    "        edges: priroshennie rebra\n",
    "    \"\"\"\n",
    "    \n",
    "    for ib, data in enumerate(train_loader):\n",
    "        e_ptr = slices[ib*batch_size:(ib+1)*batch_size+1]\n",
    "        e_ptr = e_ptr - e_ptr[0]\n",
    "        szs = e_ptr[1:]-e_ptr[:-1]\n",
    "        e_ind_start = e_ptr[1:]-szs.min()+1\n",
    "        visited_e = torch.full((e_ptr[-1],), False, dtype = torch.bool)\n",
    "        for i in range(e_ind_start.shape[0]):\n",
    "            visited_e[e_ptr[:-1][i] :e_ind_start[i]] = True # setting emask True for edges in graph\n",
    "        edges_num = torch.arange(e_ptr[-1])\n",
    "\n",
    "        visited_v = torch.full((data.ptr[-1],), False)\n",
    "        visited_v[torch.unique(data.edge_index[:, visited_e])] = True\n",
    "        \n",
    "        vert_ind = []\n",
    "        last_ind_v = torch.arange(data.ptr[-1])\n",
    "        last_ind_max = data.ptr[-1].item()        \n",
    "        ei_dict = defaultdict(set)\n",
    "        for e in data.edge_index[:, visited_e].T:\n",
    "            e = e.to(dtype=torch.long)\n",
    "            ei_dict[e[0].item()].add(e[1].item()) \n",
    "            ei_dict[e[1].item()].add(e[0].item()) \n",
    "        edge_added = []\n",
    "        \n",
    "        for i in range(data.edge_index.shape[1]): # max number of iterations, usually we stop earlier\n",
    "            if torch.all(visited_e):\n",
    "                break           \n",
    "                  \n",
    "            e1_mask = (visited_v[data.edge_index[0]] | visited_v[data.edge_index[1]]) & ~visited_e # Source in graph\n",
    "            nnedges = edges_num[e1_mask]\n",
    "            e1_ind = []\n",
    "            for j in range(1, e_ptr.shape[0]):\n",
    "                mmask = (nnedges < e_ptr[j]) & (nnedges >= e_ptr[j-1])\n",
    "                e1_ind.append(nnedges[mmask][:step])         \n",
    "            e1_ind = torch.cat(e1_ind)\n",
    "            edges_1 = data.edge_index[:, e1_ind]\n",
    "                        \n",
    "            for e in edges_1.T: \n",
    "                e_reind = []\n",
    "                edge_added.append(last_ind_v[e].view(-1,1))\n",
    "                for iv in (True,False):\n",
    "                    ind = last_ind_v[e[int(iv)]].item()\n",
    "                    if ind in ei_dict.keys():\n",
    "                        ind = last_ind_max\n",
    "                        vert_ind.append(e[int(iv)])\n",
    "                        last_ind_v[e[int(iv)]] = ind\n",
    "                        ei_dict[ind] = ei_dict[e[int(iv)].item()]   \n",
    "                        last_ind_max += 1  \n",
    "                    e_reind.append(ind)\n",
    "                    \n",
    "                ei_dict[e_reind[0]].add(e_reind[1]) #sporno\n",
    "                ei_dict[e_reind[1]].add(e_reind[0])\n",
    "        \n",
    "            # selecting source-target when both vertices in graph            \n",
    "            visited_e[e1_ind] = True  \n",
    "            visited_v[edges_1.view(-1)] = True\n",
    "        \n",
    "        edge_index = []\n",
    "        for k,v in ei_dict.items():\n",
    "            e = torch.tensor(list(v)).view(1,-1)\n",
    "            edge_index.append(torch.cat((e, torch.full_like(e, k)), dim=0))\n",
    "\n",
    "        vert_index = torch.cat((torch.arange(data.ptr[-1]),torch.Tensor(vert_ind))).to(dtype=torch.long) \n",
    "        \n",
    "        yield  data.x[vert_index], torch.cat(edge_added, dim=1),\\\n",
    "                torch.cat(edge_index, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = data_gen_e_aug(train_loader, dset.slices['edge_index'], batch_size = BSIZE,step = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itt = iter(train_gen)\n",
    "# vi, e1, graph = next(itt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "    \n",
    "class objectview(object):\n",
    "    def __init__(self, d):\n",
    "        self.__dict__ = d\n",
    "        \n",
    "arch0 = {'input_dim': dset.features.shape[1], \n",
    "        'hidden_dim': 32, \n",
    "        'block_size': 128,\n",
    "        'num_layers': 2,\n",
    "        'num_heads': 8,\n",
    "        'attn_pdrop': 0.5,\n",
    "        'resid_pdrop': 0.5,\n",
    "        'embd_pdrop': 0.5,\n",
    "        'gnn_pdrop': 0.5,\n",
    "        'num_gnn_layers': 1,\n",
    "        'mlp_pdrop': 0.5}\n",
    "conf = objectview(arch0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphNet(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_layers, dropout_p = 0.2):\n",
    "        super(GraphNet, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.batch_norms = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            self.convs.append(SAGEConv(hidden_dim, hidden_dim))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(num_features=hidden_dim))\n",
    "            \n",
    "    def forward(self, v_ind, features, edge_index):\n",
    "        x = features[v_ind]\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != self.num_layers - 1:\n",
    "                x = x.relu()\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class SRAN(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(SRAN, self).__init__()\n",
    "\n",
    "        self.input_dim = config.input_dim\n",
    "        self.hidden_dim = config.hidden_dim\n",
    "        self.block_size = config.block_size\n",
    "        self.num_layers = config.num_layers\n",
    "        self.num_heads = config.num_heads\n",
    "        self.attn_pdrop = config.attn_pdrop\n",
    "        self.resid_pdrop = config.resid_pdrop\n",
    "        self.embd_pdrop = config.embd_pdrop\n",
    "        self.gnn_pdrop  = config.gnn_pdrop\n",
    "        self.num_gnn_layers = config.num_gnn_layers\n",
    "        self.mlp_pdrop = config.mlp_pdrop\n",
    "\n",
    "        self.lin_inp = nn.Sequential(\n",
    "            nn.Linear(self.input_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(self.hidden_dim)\n",
    "#             nn.Linear(self.hidden_dim, self.hidden_dim),\n",
    "#             nn.Dropout(self.mlp_pdrop)\n",
    "        )\n",
    "\n",
    "        self.gnn = GraphNet(\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_layers=self.num_gnn_layers,\n",
    "        )\n",
    "\n",
    "        self.gpt = GPT(\n",
    "            block_size=self.block_size,\n",
    "            hidden_dim=self.hidden_dim,\n",
    "            num_layers=self.num_layers,\n",
    "            num_heads=self.num_heads,\n",
    "            attn_pdrop=self.attn_pdrop,\n",
    "            resid_pdrop=self.resid_pdrop,\n",
    "            embd_pdrop=self.embd_pdrop\n",
    "        )\n",
    "        \n",
    "    def forward(self, v_ind, features, edge_index, edges):  \n",
    "        r\"\"\" primerno napisal\"\"\"\n",
    "        features = self.lin_inp(features) \n",
    "        \n",
    "        h_gnn = self.gnn(v_ind, features, edge_index)\n",
    "        source_ind, target_ind = edges\n",
    "        h_source, h_target = gnn_emb[source_ind],gnn_emb[target_ind]\n",
    "        \n",
    "        h_next_e = self.gpt(torch.cat((h_source, h_target)))\n",
    "        \n",
    "#         y_source = self.choice(self.mlp_y_s(h_next_e), h_source) # сместить на 1 позицию\n",
    "#         z_target = self.mlp_y_t(torch.cat((h_next_e, h_source))) #а здесь не смещаить h_source?\n",
    "#         y_target = self.choice(z_target, h_target)\n",
    "        \n",
    "        return h_next_e\n",
    "        \n",
    "    def _infer(self, x, edge_index):\n",
    "        node_feat = self.mlp(x)\n",
    "\n",
    "        # if there is no edges yet -- send the first n nodes double\n",
    "        # embeddings to gpt (n - block size of gpt)\n",
    "#         if edge_index is None:\n",
    "#             gpt_input = \n",
    "\n",
    "        node_feat = self.gnn(node_feat, edge_index)\n",
    "\n",
    "    def _sample(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRAN(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eshe ne pravil!!!!\n",
    "\n",
    "def train(sampler, mod, optimizer, data, bsz):\n",
    "    mod.train()\n",
    "        \n",
    "    loss = 0   \n",
    "    encoded_features = mod.encode(data.x.to(device = mod.device))\n",
    "\n",
    "    ii=0\n",
    "    for inp in sampler(data, batch_size = bsz):\n",
    "        ii +=1\n",
    "        encoded_features = mod.encode(data.x.to(device = mod.device))\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        pred_e, targ_e, w0 = mod.iterate(data, inp, encoded, bsz) \n",
    "#         n_1.append(targ_e.sum())\n",
    "#         num_e.append(pred_e.shape[0])\n",
    "        if mod.label_smooth:\n",
    "            loss += cal_edge_loss(pred_e, targ_e)\n",
    "        else:\n",
    "            loss += F.binary_cross_entropy_with_logits(pred_e, targ_e)    \n",
    "#             loss += F.binary_cross_entropy_with_logits(pred_e, targ_e, \n",
    "#                                                        pos_weight = torch.tensor([w0], device = mod.device))    \n",
    "            \n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "#     d_train\n",
    "    train(data_gen_edges_dyn, model, optimizer, data, B_SIZE)\n",
    "#     train(data_gen_edges_dyn, model, optimizer, d_train, B_SIZE)\n",
    "#     train(data_gen_edges_dyn, model, optimizer, data, B_SIZE)\n",
    "#     acc_e_test = test( data_gen_edges_dyn, model, d_test, 1)\n",
    "#     acc_e_test = test( data_gen_edges_dyn, model, data, 1)\n",
    "#     acc = acc_e_test.mean()\n",
    "#     print(acc, np.median(acc_e_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eshe ne pravil!!!!\n",
    "@torch.no_grad()\n",
    "def test(sampler,mod, data, bsz, return_y = False, dtype = 'test'):\n",
    "    mod.eval()\n",
    "        \n",
    "    out = []\n",
    "    n_1, acc_e,num_e = [],[],[]\n",
    "    # init_state = torch.zeros((data.num_nodes*bsz,  mod.nd), device=mod.device)\n",
    "    encoded = mod.encode(data.x.to(device = mod.device))\n",
    "    indd = torch.cat(bsz*[torch.arange(data.num_nodes, device = mod.device)])\n",
    "    mod.reset_state(encoded[indd,:])\n",
    "    for inp in sampler(data, batch_size = bsz, step_max = 512, dtype = dtype):\n",
    "        pred_e, targ_e, w0 = mod.iterate(data, inp, encoded, bsz) \n",
    "        if return_y:\n",
    "            out.append((pred_e, targ_e, w0))#, dim=1\n",
    "        pred_e = torch.round(torch.sigmoid(pred_e))\n",
    "#         print('pred_e', pred_e)\n",
    "        tp_e = pred_e.eq(targ_e).sum().item()\n",
    "        n_e = pred_e.shape[0]\n",
    "        \n",
    "        n_1.append(targ_e.sum())\n",
    "\n",
    "        acc_e.append(tp_e)\n",
    "        num_e.append(n_e)\n",
    "        \n",
    "    if return_y:\n",
    "        return out\n",
    "    else:\n",
    "        return  np.array(acc_e)/(np.array(num_e)+1e-9)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genv",
   "language": "python",
   "name": "genv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
